{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiF9uQq63Gw9"
      },
      "source": [
        "# U-Net segmentation example\n",
        "### Advanced Deep Learning 2024\n",
        "This notebook was originally written by Mathias Perslev. It has been changed slightly by Christian Igel and subsequently slightly updated Stefan Sommer (mailto:sommer@di.ku.dk) and Jon Sporring (mailto:sporring@di.ku.dk).\n",
        "\n",
        "We consider the data described in:\n",
        "\n",
        "Bram van Ginneken, Mikkel B. Stegmann, Marco Loog. [Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database](https://doi.org/10.1016/j.media.2005.02.002). *Medical Image Analysis* 10(1): 19-40, 2006\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k7dGxH23GxB"
      },
      "source": [
        "## Installs\n",
        "\n",
        "On non-colab system, is usually good to make an environment and install necessary tools there. E.g., anaconda->jupyter->terminal create an environment, if you have not already, and activate it:\n",
        "```\n",
        "conda create -n adl python=3.9\n",
        "conda activate adl\n",
        "```\n",
        "then install missing packages such as:\n",
        "```\n",
        "conda install ipykernel torch matplotlib torchmetrics scikit-image jpeg\n",
        "conda install -c conda-forge segmentation-models-pytorch ipywidgets\n",
        "```\n",
        "and if you want to add it to jupyter's drop-down menu\n",
        "```\n",
        "ipython kernel install --user --name=adl\n",
        "```\n",
        "Now reload the jupyter-notebook's homepage and make a new or load an existing file. On colab, the tools have to be installed everytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RnD0QuU86FB"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "    !pip3 install torch matplotlib torchmetrics scikit-image segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0FulA5_3GxC"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp71Um2T3GxC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pformat\n",
        "from skimage.transform import resize\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from matplotlib.pyplot import imread\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yTYH2kVkeKK"
      },
      "source": [
        "## Set global device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95WD78Nu7zYW"
      },
      "outputs": [],
      "source": [
        "# GPU support?\n",
        "gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isi0fnzy3GxG"
      },
      "source": [
        "## Functions\n",
        "### Loading data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr5tW2HWZveb"
      },
      "outputs": [],
      "source": [
        "def load_npz_dataset(path, keys=('x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test')):\n",
        "    archive = np.load(path)\n",
        "    return [archive.get(key) for key in keys]\n",
        "\n",
        "def as_torch_dataset(x_arr, y_arr):\n",
        "    \"\"\"\n",
        "    Takes two numpy arrays of data points and labels (x_arr and y_arr, respectively) and\n",
        "    returns a torch TensorDataset object.\n",
        "\n",
        "    Returns: torch.utils.data.TensorDataset\n",
        "    \"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(x_arr),\n",
        "        torch.FloatTensor(y_arr)\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3lNo-Mi86FD"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCz5W51v86FE"
      },
      "outputs": [],
      "source": [
        "def plot_image_with_segmentation(image, segmentation, ax=None):\n",
        "    \"\"\"\n",
        "    Plots an image with overlayed segmentation mask. Segmentation is thresholded at 0.5\n",
        "\n",
        "    Returns: plt.fig and ax objects\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    im = image.cpu().detach().numpy()\n",
        "    segm = segmentation.cpu().detach().numpy()\n",
        "    ax.imshow(im.squeeze(), cmap=\"gray\")\n",
        "    mask = segm < 0.5\n",
        "    ax.imshow(mask.squeeze(), cmap=\"Set1\", alpha=0.5)\n",
        "    return plt.gcf(), ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJtsklqj3GxH"
      },
      "source": [
        "### EvaluatingÂ a model on data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t46YrIk86FE"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_on_single_image(model, x, device=device):\n",
        "    \"\"\"\n",
        "    Evaluate a model on a single data point on the device.\n",
        "\n",
        "    Returns: model(x)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x = x.to(device)\n",
        "        return model(x.view(1, *x.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzBstTq43GxH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, metrics_dict, reduction=True, device=device):\n",
        "    \"\"\"\n",
        "    Evaluate a model 'model' on all batches of a torch DataLoader 'data_loader'.\n",
        "    On each batch, compute all metric functions stored in dictionary 'metrics_dict'.\n",
        "\n",
        "    Returns: dict of metric_name: (list of batch-wise metrics if reduction == False, else single scalar)\n",
        "    \"\"\"\n",
        "\n",
        "    # defaultdict(list) returns a dictionary-like object with default_factory list.\n",
        "    # When a new key is encountered, an entry is automatically created of type default_factory.\n",
        "    metrics = defaultdict(list)\n",
        "    with torch.no_grad():\n",
        "        for i, (batch_x, batch_y) in enumerate(data_loader):\n",
        "            # Predict on batch\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            logits = model(batch_x)\n",
        "\n",
        "            # Compute all metrics\n",
        "            for metric_name, metric_func in metrics_dict.items():\n",
        "                value = metric_func(logits.cpu(), batch_y.cpu()).item() #.cpu().numpy()\n",
        "                metrics[metric_name].append(value)\n",
        "\n",
        "    if reduction == True:\n",
        "        # Return mean values\n",
        "        return {key: np.mean(value) for key, value in metrics.items()}\n",
        "    else:\n",
        "        return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cSZX0bq86FF"
      },
      "source": [
        "### Saving and loading model and optimizer state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jOELTgQ86FF"
      },
      "outputs": [],
      "source": [
        "def save_model(model, path, optimizer=None):\n",
        "    \"\"\"\n",
        "    Saves the state_dict of a torch model and optional optimizer to 'path'\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    state = {\"model\": model.state_dict()}\n",
        "    if optimizer is not None:\n",
        "        state[\"optimizer\"] = optimizer.state_dict()\n",
        "    torch.save(state, path)\n",
        "\n",
        "\n",
        "def load_model(model, path, optimizer=None):\n",
        "    \"\"\"\n",
        "    Loads the state_dict of a torch model and optional optimizer from 'path'\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    state = torch.load(path)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(state[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E23GGXNl86FF"
      },
      "source": [
        "### Plotting training/validation histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwdENGGQ86FF"
      },
      "outputs": [],
      "source": [
        "def plot_histories(train_history=None, val_history=None, label=\"Loss\"):\n",
        "    \"\"\"\n",
        "    Takes a list of training and/or validation metrics and plots them\n",
        "    Returns: plt.figure and ax objects\n",
        "    \"\"\"\n",
        "    if not train_history and not val_history:\n",
        "        raise ValueError(\"Must specify at least one of 'train_histories' and 'val_histories'\")\n",
        "    fig = plt.figure(figsize=(5, 3))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    epochs = np.arange(len(train_history or val_history))\n",
        "    if train_history:\n",
        "        ax.plot(epochs, train_history, label=\"Training\", color=\"black\")\n",
        "    if val_history:\n",
        "        ax.plot(epochs, val_history, label=\"Validation\", color=\"darkred\")\n",
        "\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(label)\n",
        "    ax.legend(loc=0)\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPW7_p1c3GxH"
      },
      "source": [
        "### Main training loop\n",
        "\n",
        "We want to track the F1 score during training. This generates some additional code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpq0Qknp3GxH"
      },
      "outputs": [],
      "source": [
        "def run_one_epoch(model, loss, optimizer, train_loader, val_loader, n_epochs, metrics_dict, device=device):\n",
        "    \"\"\"\n",
        "    Run 1 epoch of training\n",
        "    Changes to model parameters and optimizer occour internally (state updates)\n",
        "    Returns:\n",
        "        two dictionaries, training and a validation metrics\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        # Zero out stored gradients for all parameters\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"   -- Batch {i+1}/{len(train_loader)}\", end=\" / \")\n",
        "        # Predict on batch\n",
        "        logits = model(batch_x)\n",
        "\n",
        "        # Compute loss function\n",
        "        loss_tensor = loss(logits, batch_y)\n",
        "        loss_scalar = loss_tensor.detach().cpu().numpy()\n",
        "        train_losses.append(loss_scalar)\n",
        "        print(\"Loss: \", loss_scalar)\n",
        "\n",
        "        # Backprop and step\n",
        "        loss_tensor.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Run validation\n",
        "    print(\"   Validation running...\")\n",
        "    val_metrics = evaluate_model(\n",
        "        model=model,\n",
        "        data_loader=val_loader,\n",
        "        metrics_dict=metrics_dict\n",
        "    )\n",
        "    # Return loss and metrics as dicts\n",
        "    return {\"loss\": np.mean(train_losses)}, val_metrics\n",
        "\n",
        "\n",
        "def merge_list_of_dicts(list_of_dicts):\n",
        "    \"\"\"\n",
        "    Takes a list of dictionaries and merges them into a single dictionary pointing to lists\n",
        "\n",
        "    E.g. [{\"loss\": 5}, {\"loss\": 3}, {\"loss\": -2, \"F1\": 0.5}] --> {\"loss\": [5, 3, -2], \"F1\": [0.5]}\n",
        "\n",
        "    Returns: dict\n",
        "    \"\"\"\n",
        "    merged = defaultdict(list)\n",
        "    for dict_ in list_of_dicts:\n",
        "        for value, key in dict_.items():\n",
        "            merged[value].append(key)\n",
        "    return merged\n",
        "\n",
        "\n",
        "def training_loop(model, loss, optimizer, train_loader, val_loader, n_epochs, init_epoch=None, metrics_dict=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Run training of a model given a loss function, optimizer and a set of training and validation data.\n",
        "    Supports computing additional metrics on the validation set (only) via the metrics_dict param.\n",
        "    Specify save_path to store the model at each epoch.\n",
        "\n",
        "    Returns:\n",
        "        Two lists of metric dictionaries for each epoch for training and validation, specifically\n",
        "    \"\"\"\n",
        "    train_history, val_history = [], []\n",
        "\n",
        "    metrics_with_loss = {\"loss\": loss}\n",
        "    if metrics_dict is not None:\n",
        "        metrics_with_loss.update(metrics_dict)\n",
        "\n",
        "    if init_epoch == None:\n",
        "        init_epoch = 0\n",
        "    try:\n",
        "        for i in range(init_epoch, n_epochs):\n",
        "            print(f\"Epoch {i+1}/{n_epochs}\")\n",
        "            train_metrics, val_metrics = run_one_epoch(\n",
        "                model=model,\n",
        "                loss=loss,\n",
        "                optimizer=optimizer,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                n_epochs=n_epochs,\n",
        "                metrics_dict=metrics_with_loss\n",
        "            )\n",
        "            print(\"   Mean epoch metrics:\")\n",
        "            print(f\"   Training:   {pformat(train_metrics)}\")\n",
        "            print(f\"   Validation: {pformat(val_metrics)}\")\n",
        "            train_history.append(train_metrics), val_history.append(val_metrics)\n",
        "\n",
        "            if save_path:\n",
        "                save_path_epoch = f\"epoch_{i+1}_{save_path}\"\n",
        "                print(f\"   Saving to: {save_path_epoch}\")\n",
        "                save_model(model, save_path_epoch, optimizer)\n",
        "                old_path_epoch = f\"epoch_{i}_{save_path}\"\n",
        "                if os.path.exists(old_path_epoch):\n",
        "                    os.remove(old_path_epoch)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Training stopped.\")\n",
        "        pass\n",
        "\n",
        "    # Merge list of training and validation dicts into single dicts\n",
        "    return merge_list_of_dicts(train_history), merge_list_of_dicts(val_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vzrPKEQWBGc"
      },
      "source": [
        "## Main program\n",
        "### Mount Google drive when using Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GoilmVm3Pts",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# If you are working from Colab, better mount your google drive and change directory appropriately.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive/')\n",
        "    os.chdir('gdrive/MyDrive/Colab Notebooks/ADL/assignment1')\n",
        "except:\n",
        "    print('Google drive not mounted')\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oII1ZIcT3GxE"
      },
      "source": [
        "### Setup database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFjgYdp3GxE"
      },
      "source": [
        "Load database with chest X-rays with lung segmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM393wDKXae-"
      },
      "outputs": [],
      "source": [
        "data_root='./datasets'\n",
        "data_npz='lung_field_dataset.npz'\n",
        "data_fn = os.path.join(data_root, \"lung_field_dataset.npz\")\n",
        "force_download = False\n",
        "\n",
        "if (not os.path.exists(data_fn)) or force_download:\n",
        "  download_url(\"https://sid.erda.dk/share_redirect/gCTc6o3KAh\", data_root, data_npz)\n",
        "else:\n",
        "  print('Using existing', data_fn)\n",
        "\n",
        "# Load train/val/test data\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = load_npz_dataset(data_fn)\n",
        "# Bring images into PyTorch format\n",
        "x_train = np.moveaxis(x_train, 3, 1)\n",
        "y_train = np.moveaxis(y_train, 3, 1)\n",
        "x_val = np.moveaxis(x_val, 3, 1)\n",
        "y_val = np.moveaxis(y_val, 3, 1)\n",
        "x_test = np.moveaxis(x_test, 3, 1)\n",
        "y_test = np.moveaxis(y_test, 3, 1)\n",
        "\n",
        "print(\"x train:\", x_train.shape)\n",
        "print(\"y train:\", y_train.shape)\n",
        "print(\"x val: \", x_val.shape)\n",
        "print(\"y val: \", y_val.shape)\n",
        "print(\"x test:\", x_test.shape)\n",
        "print(\"y test:\", y_test.shape)\n",
        "\n",
        "# Init torch datasets\n",
        "train_dataset = as_torch_dataset(x_train, y_train)\n",
        "val_dataset = as_torch_dataset(x_val, y_val)\n",
        "test_dataset = as_torch_dataset(x_test, y_test)\n",
        "\n",
        "# Init dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Plot an example\n",
        "fig, ax = plot_image_with_segmentation(*train_dataset[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8z2VvB83GxI"
      },
      "source": [
        "### Init model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjrw4VsE3GxI"
      },
      "outputs": [],
      "source": [
        "# Init U-Net model\n",
        "model = smp.Unet(encoder_name='efficientnet-b0',in_channels=1,classes=1)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nXGVTLw3GxI"
      },
      "source": [
        "### Continue training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b2g5WUq3GxI"
      },
      "outputs": [],
      "source": [
        "# Specify integer, starting at 1\n",
        "init_epoch = None\n",
        "if init_epoch != None:\n",
        "    load_model(model, f\"epoch_{init_epoch}_model.ckpt\", optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrxO77vW3GxI"
      },
      "source": [
        "### Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWETbe5d3GxJ"
      },
      "outputs": [],
      "source": [
        "# Define loss and metrics\n",
        "loss = torch.nn.MSELoss(reduction=\"mean\")\n",
        "metrics = {\"f1\": torchmetrics.classification.F1Score(task='binary', num_classes=1, average=\"macro\")}\n",
        "\n",
        "# Run training\n",
        "train_history, val_history = training_loop(\n",
        "    model=model,\n",
        "    loss=loss,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    init_epoch=init_epoch,\n",
        "    n_epochs=100,\n",
        "    metrics_dict=metrics,\n",
        "    save_path=\"model.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WHHZc5K3GxJ"
      },
      "source": [
        "### Plot training and validation histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxBzn18s3GxJ"
      },
      "outputs": [],
      "source": [
        "plot_histories(train_history['loss'], val_history['loss'], label=\"Loss\")\n",
        "plot_histories(train_history=None, val_history=val_history['f1'], label=\"F1 Score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFD4Yrxy3GxJ"
      },
      "source": [
        "### Evaluate on single test-set image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncio8D4S3GxJ"
      },
      "outputs": [],
      "source": [
        "# Select a test image\n",
        "x,y = test_dataset[0]\n",
        "pred = evaluate_model_on_single_image(model, x)\n",
        "\n",
        "# Plot the result\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 15))\n",
        "ax1.set_title(\"True mask\")\n",
        "ax2.set_title(\"Predicted mask\")\n",
        "plot_image_with_segmentation(x, y, ax=ax1)\n",
        "plot_image_with_segmentation(x, pred, ax=ax2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbWED8IJ3GxJ"
      },
      "source": [
        "### Evaluate on whole test-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgPqlai_3GxK"
      },
      "outputs": [],
      "source": [
        "# OBS: Returns batch-wise metrics, but test_loader has batch_size = 1\n",
        "f1_test_scores = evaluate_model(model, test_loader, metrics, reduction=False)[\"f1\"]\n",
        "\n",
        "print(\"Test cases:\", len(f1_test_scores))\n",
        "print(\"Mean F1:   \", np.mean(f1_test_scores))\n",
        "print(\"STD  F1:   \", np.std(f1_test_scores))\n",
        "print(\"Min. F1:   \", np.min(f1_test_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qZqkTsF86FJ"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt5Abwsl_XvG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "adl",
      "language": "python",
      "name": "adl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}