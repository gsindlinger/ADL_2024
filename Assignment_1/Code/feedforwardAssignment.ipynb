{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rG1ZnPe8QKs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch import Tensor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Iu4BPSQ93e"
      },
      "outputs": [],
      "source": [
        "print(\"Using torch\", torch.__version__)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # prefer gpu\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZT9APicLrpA"
      },
      "outputs": [],
      "source": [
        "class XORDataset(data.Dataset):\n",
        "    def __init__(self, size, std=0.1, device=device):\n",
        "        \"\"\"Make a random XOR data set.\n",
        "\n",
        "        Args:\n",
        "            size: Number of data points we want to generate\n",
        "            std: Standard deviation of the noise (see generate_continuous_xor function)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.std = std\n",
        "        self.generate_continuous_xor()\n",
        "\n",
        "    def generate_continuous_xor(self):\n",
        "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
        "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
        "        # If x=y, the label is 0.\n",
        "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
        "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
        "        data += self.std * torch.randn(data.shape)\n",
        "        # Move data to GPU if relevant\n",
        "        self.data = data.to(device)\n",
        "        self.label = label.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label\n",
        "\n",
        "    def toNumpy(self):\n",
        "        return (self.data.cpu().numpy(), self.label.cpu().numpy()) # must be on cpy to convert to numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K6t-1Po5PVz"
      },
      "outputs": [],
      "source": [
        "def visualize_binary_samples(dataset):\n",
        "    \"\"\"visualize a data set with binary labels.\n",
        "\n",
        "    Args:\n",
        "        data: the data set as, e.g., generated by XORDataset\n",
        "    \"\"\"\n",
        "    (data,label) = dataset.toNumpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.scatter(data_0[:, 0], data_0[:, 1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:, 0], data_1[:, 1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()\n",
        "\n",
        "dataset = XORDataset(size=200)\n",
        "print(\"Size of dataset:\", len(dataset))\n",
        "print(\"Data point 0:\", dataset[0])\n",
        "visualize_binary_samples(dataset)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXgYWJWDTyC1"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, src, tg, depth, width, device=device):\n",
        "        \"\"\"Make a convolutional neural network. Last layer uses the identity transfer function, the rest use the tanh.\n",
        "\n",
        "        Args:\n",
        "            src: The number of input nodes\n",
        "            tg: The number of output nodes\n",
        "            depth: The number of hidden layers\n",
        "            width: The width of the hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_sizes = [src, tg, depth, width]\n",
        "\n",
        "\n",
        "        ###############################################################################################\n",
        "        # Assignment: replace self.model with your code for a network src-width-width-...-tg network\n",
        "        ###############################################################################################\n",
        "        functionLst = [nn.Linear(src, width), nn.Identity()]\n",
        "        self.model = nn.Sequential(*functionLst) #unpacking with the '*' operator\n",
        "\n",
        "        # Move data to GPU if relevant\n",
        "        self.model.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.squeeze(dim=1) # flat\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
        "    \"\"\"Train a model.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        optimizer: The optimizer to use\n",
        "        data_loader: The function to load the data\n",
        "        loss_module: The function to evaluate the loss function\n",
        "        num_epochs: The number of epochs to train\n",
        "    \"\"\"\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "            # Step 1: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "            #preds = preds.squeeze(dim=1)  # Output is [Batch size, 1], but we want [Batch size]\n",
        "\n",
        "            # Step 2: Calculate the loss\n",
        "            loss = loss_module(preds, data_labels.float())\n",
        "\n",
        "            # Step 3: Perform backpropagation\n",
        "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
        "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
        "            optimizer.zero_grad()\n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Step 4: Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "def eval_model(model, data_loader):\n",
        "    \"\"\"Evaluate a model.\n",
        "\n",
        "    Args:\n",
        "        model: A (trained) model\n",
        "        data_loader: The function to load the data\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to eval mode\n",
        "    true_preds, num_preds = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():  # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "            preds = model(data_inputs)\n",
        "            preds = torch.sigmoid(preds)  # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long()  # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_labels).sum()\n",
        "            num_preds += data_labels.shape[0]\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5oXnSxVXS7B"
      },
      "outputs": [],
      "source": [
        "###############################################################################################\n",
        "# Assignment: replace the following with code that trains your networks repeatedly on the same\n",
        "# XORDataset for a range of depths and widths, e.g., depths and widths in the range range(4)\n",
        "# and each repeated 3 times.\n",
        "###############################################################################################\n",
        "\n",
        "loss_module = nn.BCEWithLogitsLoss()\n",
        "train_dataset = XORDataset(size=1000)\n",
        "train_data_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_dataset = XORDataset(size=500)\n",
        "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
        "test_data_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)\n",
        "\n",
        "# Setup a feed-forward network with variable width\n",
        "model = SimpleClassifier(src=2, tg=1, depth=0, width=0)\n",
        "print(model)\n",
        "\n",
        "# Train\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "train_model(model, optimizer, train_data_loader, loss_module, num_epochs=200)\n",
        "\n",
        "# Evaluate\n",
        "acc = eval_model(model, test_data_loader)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACzCJ7pFSwoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "adl",
      "language": "python",
      "name": "adl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}